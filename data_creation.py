# -*- coding: utf-8 -*-
"""Data_Creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1boCzwP4xJxH02sdacWTNj3MtO_LSxYtS

# Preprocessing and Sentiment labelling
"""

# Install required libraries
!pip install numpy pandas matplotlib seaborn scikit-learn nltk wordcloud

# Import libraries
import pandas as pd
import numpy as np
import nltk
import nltk
nltk.download('punkt')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re

# Download NLTK data
nltk.download('vader_lexicon')
nltk.download('stopwords')

# Load Twitter data from CSV
file_path = "Output_tweet_Google (1).csv"  # Replace with the actual path to your CSV file
twitter_data = pd.read_csv(file_path)

# Display the first few rows of the dataset
twitter_data.head()

# Preprocessing function
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+', '', text)

    # Remove mentions and hashtags
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)

    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Tokenization and stemming
    stop_words = set(stopwords.words('english'))
    tokens = nltk.word_tokenize(text)
    ps = PorterStemmer()
    stemmed_tokens = [ps.stem(token) for token in tokens if token not in stop_words]

    # Join tokens back into a sentence
    preprocessed_text = ' '.join(stemmed_tokens)

    return preprocessed_text

# Apply preprocessing to the 'tweet' column
twitter_data['processed_tweet'] = twitter_data['Tweets'].apply(preprocess_text)

# Display the preprocessed tweets
twitter_data[['Date', 'processed_tweet']].head()

# Calculate TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(twitter_data['processed_tweet'])

# Extract feature names
feature_names = tfidf_vectorizer.get_feature_names_out()

# Convert TF-IDF matrix to DataFrame
tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)

# Display the TF-IDF DataFrame
tfidf_df.head()

# Calculate sentiment using VADER
analyzer = SentimentIntensityAnalyzer()
twitter_data['Sentiment_Score'] = twitter_data['processed_tweet'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

# Display the sentiment column
twitter_data[['processed_tweet', 'Sentiment_Score']].head()

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(tfidf_df.sum().to_dict())

# Display the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

df_sentiment = twitter_data
df_sentiment

"""# Merge with the finance data"""

for i in range(len(df_sentiment)):
  x = df_sentiment['Date'][i].split(' ')
  df_sentiment['Date'][i] = x[0]

dates = df_sentiment['Date'].unique()

date_dict = {date: 0 for date in dates}

print(date_dict)

for i in range(len(df_sentiment)):
  score = df_sentiment['Sentiment_Score'][i]
  date_dict[df_sentiment['Date'][i]] += score

date_dict

df = pd.read_csv('GOOG.csv')

df['Date'][0]

# Convert the dictionary to a DataFrame
df_to_merge = pd.DataFrame(list(date_dict.items()), columns=['Date', 'Avg_Sentiment_Score'])

# Merge based on the 'key' column
merged_df = pd.merge(df, df_to_merge, on='Date', how='left')

merged_df

# Drop rows with NaN values
df = merged_df.dropna()

df['Avg_Sentiment_Score'] /= 20

df

"""# Using TA package for getting more features"""

!pip install ta

# Install the ta library if you haven't already
# pip install ta

import pandas as pd
import ta

# Sample financial data


# Convert the 'date' column to datetime
#data['date'] = pd.to_datetime(data['date'])

# Add technical analysis features
data = ta.add_all_ta_features(df, open="Open", high="High", low="Low", close="Close", volume="Volume", fillna=True)

# Display the DataFrame with added features
print(data)

data

"""# Labels (adj closing price) for the next day and 14 day prediction windows"""

data = data.reset_index()

data['Label'] = 0
for i in range(len(data)-1):
  data['Label'][i] = data['Adj Close'][i+1]  # next data

data = data[:len(data)-1]

data.to_csv('Next_day_pred_GOOG.csv')

data['Label'] = 0
for i in range(len(data)-14):
  data['Label'][i] = data['Adj Close'][i+14]  # next data (14 day window)

data = data[:len(data)-14]
data.to_csv('14_day_pred_GOOG.csv')

